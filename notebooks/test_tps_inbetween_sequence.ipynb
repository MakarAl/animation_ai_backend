{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPS Inbetween Sequence Demo\n",
    "\n",
    "This notebook demonstrates how to use the TPSInbetweenWrapper to generate inbetween frames at arbitrary timesteps between two keyframes. Now with GIF and image size options!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../wrappers')  # Adjust if needed\n",
    "from tps_inbetween_wrapper import TPSInbetweenWrapper\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import os\n",
    "from IPython.display import Image as IPyImage, display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up your image paths, timesteps, and options\n",
    "Edit these variables to point to your own keyframes and choose the desired inbetween timesteps and options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Replace with your own image paths\n",
    "img0_path = '../input_images/ep01_sc324_Cyberslav_CLEAN_0007.jpg'\n",
    "img1_path = '../input_images/ep01_sc324_Cyberslav_CLEAN_0008.jpg'\n",
    "output_dir = '../output_images'\n",
    "timesteps = [0.25, 0.5, 0.75]  # You can change this to e.g. [0.33, 0.66] or more\n",
    "\n",
    "# New options\n",
    "create_gif = True\n",
    "gif_duration = 0.3  # seconds per frame\n",
    "max_image_size = 1080  # Resize input images to this max dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize the TPSInbetweenWrapper\n",
    "You can enable or disable vector cleanup, uniform thickness, etc. as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPS-Inbetween wrapper initialized with device: cpu\n",
      "Edge sharpening: enabled\n",
      "Vector cleanup: enabled\n",
      "Uniform thickness: disabled\n",
      "Loading matching model (GlueStick)...\n",
      "Loading TPS-Inbetween model...\n",
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "wrapper = TPSInbetweenWrapper(\n",
    "    use_cpu=True,  # Set to False to use GPU if available\n",
    "    no_edge_sharpen=False,\n",
    "    vector_cleanup=True,\n",
    "    uniform_thin=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate inbetween frames at specified timesteps (and optionally a GIF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating matches between images...\n",
      "Resized image from 3840x2160 to 1080x607 to prevent memory issues\n",
      "Resized image from 3840x2160 to 1080x607 to prevent memory issues\n",
      "Generating 3 inbetween frames at t=[0.25, 0.5, 0.75] ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (607) must match the size of tensor b (608) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_paths, gif_path \u001b[38;5;241m=\u001b[39m wrapper\u001b[38;5;241m.\u001b[39minterpolate_sequence(\n\u001b[1;32m      2\u001b[0m     img0_path,\n\u001b[1;32m      3\u001b[0m     img1_path,\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[1;32m      5\u001b[0m     timesteps\u001b[38;5;241m=\u001b[39mtimesteps,\n\u001b[1;32m      6\u001b[0m     create_gif\u001b[38;5;241m=\u001b[39mcreate_gif,\n\u001b[1;32m      7\u001b[0m     gif_duration\u001b[38;5;241m=\u001b[39mgif_duration,\n\u001b[1;32m      8\u001b[0m     max_image_size\u001b[38;5;241m=\u001b[39mmax_image_size\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerated frames:\u001b[39m\u001b[38;5;124m'\u001b[39m, output_paths)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gif_path:\n",
      "File \u001b[0;32m/Volumes/data_vault/Animation AI/Animation_AI_backend/notebooks/../wrappers/tps_inbetween_wrapper.py:602\u001b[0m, in \u001b[0;36mTPSInbetweenWrapper.interpolate_sequence\u001b[0;34m(self, img0_path, img1_path, output_dir, timesteps, temp_dir, create_gif, gif_duration, max_image_size)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(timesteps)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inbetween frames at t=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     pred, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbetween_model(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m torch_gray0, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m torch_gray1, [matches_path])\n\u001b[1;32m    604\u001b[0m pred \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_edge_sharpen(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m p) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_sharpen \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pred]\n\u001b[1;32m    606\u001b[0m \u001b[38;5;66;03m# Save individual frames\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/data_vault/Animation AI/Animation_AI_backend/models/TPS_Inbetween/model/tpsinbet.py:108\u001b[0m, in \u001b[0;36mTPS_inbet.forward\u001b[0;34m(self, x0, x1, matches_path, middle, aug)\u001b[0m\n\u001b[1;32m    105\u001b[0m fine_I1t \u001b[38;5;241m=\u001b[39m warp(coarse_I1t, flow_bet_coar[:, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m4\u001b[39m])\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# synthesize frames\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m refine_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet(x0, x1, coarse_flow0t\u001b[38;5;241m+\u001b[39mflow_bet_coar[:, :\u001b[38;5;241m2\u001b[39m], coarse_flow1t\u001b[38;5;241m+\u001b[39mflow_bet_coar[:, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m4\u001b[39m], fine_I0t, fine_I1t, m, feats0, feats1)\n\u001b[1;32m    109\u001b[0m res \u001b[38;5;241m=\u001b[39m refine_out[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m, :, :]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Crop all tensors to min size before addition\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (607) must match the size of tensor b (608) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "output_paths, gif_path = wrapper.interpolate_sequence(\n",
    "    img0_path,\n",
    "    img1_path,\n",
    "    output_dir=output_dir,\n",
    "    timesteps=timesteps,\n",
    "    create_gif=create_gif,\n",
    "    gif_duration=gif_duration,\n",
    "    max_image_size=max_image_size\n",
    ")\n",
    "print('Generated frames:', output_paths)\n",
    "if gif_path:\n",
    "    print('GIF saved to:', gif_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display the generated inbetween frames inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(output_paths), figsize=(4*len(output_paths), 4))\n",
    "if len(output_paths) == 1:\n",
    "    axes = [axes]\n",
    "for ax, path, t in zip(axes, output_paths, timesteps):\n",
    "    img = Image.open(path)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f't = {t:.2f}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (Optional) Display the GIF inline if created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gif_path:\n",
    "    with open(gif_path, 'rb') as f:\n",
    "        display(IPyImage(data=f.read()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
